{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When to use `dask.dataframe`\n",
    "\n",
    "* We have seen DataFrames as list of `delayed` objects\n",
    "* Now: treat them as a single DataFrame\n",
    "* Pandas is great for tabular datasets that fit in memory.\n",
    "* Dask becomes useful when the dataset is larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `dask.dataframe` basics\n",
    "\n",
    "* `dask.dataframe` implements a blocked parallel `DataFrame`\n",
    "* mimics a large subset of the Pandas `DataFrame`.\n",
    "* One operation on a Dask `DataFrame` triggers many pandas operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Related Documentation\n",
    "\n",
    "*  [Dask DataFrame documentation](http://dask.pydata.org/en/latest/dataframe.html)\n",
    "*  [Pandas documentation](http://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main Take-aways\n",
    "\n",
    "1.  Dask.dataframe should be familiar to Pandas users\n",
    "2.  The partitioning of dataframes is important for efficient queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create artifical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "names = ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank', 'George',\n",
    "'Hannah', 'Ingrid', 'Jerry', 'Kevin', 'Laura', 'Michael', 'Norbert', 'Oliver',\n",
    "'Patricia', 'Quinn', 'Ray', 'Sarah', 'Tim', 'Ursula', 'Victor', 'Wendy',\n",
    "'Xavier', 'Yvonne', 'Zelda']\n",
    "\n",
    "k = 100\n",
    "\n",
    "\n",
    "def account_params(k):\n",
    "    ids = np.arange(k, dtype=int)\n",
    "    names2 = np.random.choice(names, size=k, replace=True)\n",
    "    wealth_mag = np.random.exponential(100, size=k)\n",
    "    wealth_trend = np.random.normal(10, 10, size=k)\n",
    "    freq = np.random.exponential(size=k)\n",
    "    freq /= freq.sum()\n",
    "\n",
    "    return ids, names2, wealth_mag, wealth_trend, freq\n",
    "\n",
    "def account_entries(n, ids, names, wealth_mag, wealth_trend, freq):\n",
    "    indices = np.random.choice(ids, size=n, replace=True, p=freq)\n",
    "    amounts = ((np.random.normal(size=n) + wealth_trend[indices])\n",
    "                                         * wealth_mag[indices])\n",
    "\n",
    "    return pd.DataFrame({'id': indices,\n",
    "                         'names': names[indices],\n",
    "                         'amount': amounts.astype('i4')},\n",
    "                         columns=['id', 'names', 'amount'])\n",
    "\n",
    "\n",
    "def accounts(n, k):\n",
    "    ids, names, wealth_mag, wealth_trend, freq = account_params(k)\n",
    "    df = account_entries(n, ids, names, wealth_mag, wealth_trend, freq)\n",
    "    return df\n",
    "\n",
    "\n",
    "def json_entries(n, *args):\n",
    "    df = account_entries(n, *args)\n",
    "    g = df.groupby(df.id).groups\n",
    "\n",
    "    data = []\n",
    "    for k in g:\n",
    "        sub = df.iloc[g[k]]\n",
    "        d = dict(id=int(k), name=sub['names'].iloc[0],\n",
    "                transactions=[{'transaction-id': int(i), 'amount': int(a)}\n",
    "                              for i, a in list(zip(sub.index, sub.amount))])\n",
    "        data.append(d)\n",
    "\n",
    "    return data\n",
    "\n",
    "def accounts_json(n, k):\n",
    "    args = account_params(k)\n",
    "    return json_entries(n, *args)\n",
    "\n",
    "def accounts_csvs(num_files, n, k):\n",
    "    fn = os.path.join(data_dir, 'accounts.%d.csv' % (num_files - 1))\n",
    "\n",
    "    if os.path.exists(fn):\n",
    "        return\n",
    "\n",
    "    print(\"Create CSV accounts for dataframe exercise\")\n",
    "\n",
    "    args = account_params(k)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        df = account_entries(n, *args)\n",
    "        df.to_csv(os.path.join(data_dir, 'accounts.%d.csv' % i),\n",
    "                  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "accounts_csvs(3, 1000000, 500)\n",
    "\n",
    "import os\n",
    "import dask\n",
    "filename = os.path.join('data', 'accounts.*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read data into a DataFrame\n",
    "\n",
    "This works just like `pandas.read_csv`, except on multiple csv files at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "# load and count number of rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What happened here?\n",
    "\n",
    "- Dask investigated the input path and found that there are three matching files \n",
    "- a set of jobs was intelligently created for each chunk - one per original CSV file in this case\n",
    "- each file was loaded into a pandas dataframe, had `len()` applied to it\n",
    "- the subtotals were combined to give you the final grant total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real Data\n",
    "\n",
    "Lets try this with the New York Taxi Data we just converted to Apache Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(os.path.join('taxi-data-parquet/', '*.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the respresentation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can view the start and end of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computations with `dask.dataframe`\n",
    "\n",
    "We compute the maximum of the `trip_distance` column. With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    df = pd.read_parquet(fn)\n",
    "    maxes.append(df.trip_distance.max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df['trip_distance'].max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.  As with `dask.delayed`, we need to call `.compute()` when we're done.  Up until this point everything is lazy.\n",
    "2.  Dask will delete intermediate results as soon as possible.\n",
    "    -  Lets us handle datasets that are larger than memory\n",
    "    -  Repeated computations will have to load all of the data in each time\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As with `Delayed` objects, you can view the underlying task graph using the `.visualize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the parallelism\n",
    "df['trip_distance'].max().visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparision to Pandas\n",
    "\n",
    "* Pandas is more mature and fully featured\n",
    "* If your data fits in memory then you should use Pandas.\n",
    "* When you see `MemoryError: `, then it's time for `dask`.\n",
    "* When you want more than 1 CPU, also use `dask`    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance comparison\n",
    "\n",
    "Dask.dataframe operations use `pandas` operations internally, but:\n",
    "\n",
    "1. Dask introduces overhead, around 1ms per task (negligible)\n",
    "2. When Pandas releases the GIL (coming to `groupby` in the next version): in-process parallelism\n",
    "3. With the GIL, we need several processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dask DataFrame Data Model\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet('taxi-data-parquet/*.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['trip_distance'].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['trip_distance'].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tasks = df.groupby('passenger_count')['trip_distance'].mean()\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tasks.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tasks.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df['VendorID'].nunique().visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df['VendorID'].nunique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VendorID'].unique().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Custom code and Dask Dataframe\n",
    "\n",
    "`dask.dataframe` provides a few methods to make applying custom functions to Dask DataFrames easier:\n",
    "\n",
    "- [`map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)\n",
    "- [`map_overlap`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_overlap)\n",
    "- [`reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the docs for `map_partitions`\n",
    "\n",
    "help(df['trip_distance'].map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to apply a function that operates on a DataFrame to each partition.\n",
    "In this case, we'll apply `pd.to_timedelta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_time(df):\n",
    "    return df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "\n",
    "duration = df.map_partitions(trip_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `dask.dataframe` limitations\n",
    "\n",
    "Dask.dataframe only covers a small but well-used portion of the Pandas API.\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n",
    "\n",
    "Operations like ``set_index`` work, but are slower than in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What definitely works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Trivially parallelizable operations (fast):\n",
    "    *  Elementwise operations:  ``df.x + df.y``\n",
    "    *  Row-wise selections:  ``df[df.x > 0]``\n",
    "    *  Loc:  ``df.loc[4.0:10.5]``\n",
    "    *  Common aggregations:  ``df.x.max()``\n",
    "    *  Is in:  ``df[df.x.isin([1, 2, 3])]``\n",
    "    *  Datetime/string accessors:  ``df.timestamp.month``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Cleverly parallelizable operations (also fast):\n",
    "    *  groupby-aggregate (with common aggregations): ``df.groupby(df.x).y.max()``\n",
    "    *  value_counts:  ``df.x.value_counts``\n",
    "    *  Drop duplicates:  ``df.x.drop_duplicates()``\n",
    "    *  Join on index:  ``dd.merge(df1, df2, left_index=True, right_index=True)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Operations requiring a shuffle (slow-ish, unless on index)\n",
    "    *  Set index:  ``df.set_index(df.x)``\n",
    "    *  groupby-apply (with anything):  ``df.groupby(df.x).apply(myfunc)``\n",
    "    *  Join not on the index:  ``pd.merge(df1, df2, on='name')``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ingest operations\n",
    "    *  Files: ``dd.read_csv, dd.read_parquet, dd.read_json, dd.read_orc``, etc.\n",
    "    *  Pandas: ``dd.from_pandas``\n",
    "    *  Anything supporting numpy slicing: ``dd.from_array``\n",
    "    *  From any set of functions creating sub dataframes via ``dd.from_delayed``.\n",
    "    *  Dask.bag: ``mybag.to_dataframe(columns=[...])``"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
